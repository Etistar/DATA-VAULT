# Use Case â€“ Retail Promotion Analytics avec Data Vault & Power BI

ğŸ‘¤ Auteur : Etienne ZONON  
ğŸ“… Date : Juillet 2025  
ğŸ§° Technologies : Microsoft Fabric (Lakehouse, Notebooks), Data Vault 2.0, Power BI, SQL, PySpark, Python

---

## Sommaire

- [1. Contexte](#1--contexte)
- [2. DonnÃ©es utilisÃ©es et arborescence](#2--donnÃ©es-utilisÃ©es-et-aborescence)
- [3. ModÃ©lisation Data Vault](#3--modÃ©lisation-data-vault)
  - [3.1 Hubs](#31--hubs)
  - [3.2 Links](#32--links)
  - [3.3 Satellites](#33--satellites)
- [4. Pipelines & Ingestion SQL](#4--pipelines--ingestion-sql)
- [5. Business Vault](#5--business-vault)
- [6. Vue analytique dans Power BI](#6--vue-analytique-dans-power-bi)
- [7. Cas dâ€™usage & KPI crÃ©Ã©s](#7--cas-dusage--kpi-crÃ©Ã©s)
- [8. Prochaines Ã©tapes](#8--prochaines-Ã©tapes)

---

## 1. Contexte

Ce projet simule une analyse promotionnelle dans un contexte retail Ã  lâ€™aide dâ€™une architecture moderne (Data Vault 2.0) hÃ©bergÃ©e sur Microsoft Fabric.  
Il permet de suivre les performances de ventes en lien avec les campagnes marketing et de construire des KPI fiables dans Power BI.

AprÃ¨s discussion avec le mÃ©tier, il dÃ©sire suivre lâ€™impact des promotions sur les ventes hebdomadaires, rÃ©gionales et produits.
Ensemble, il a Ã©tÃ© dÃ©fini:

KPI demandÃ©s :
- CA total par semaine et rÃ©gion
- CA sous promo vs sans promo
- ROI marketing par semaine
- Top 5 produits promo

Filtrage :
- Par rÃ©gion
- Par produit
- Par pÃ©riode
- Par promo_flag (promo ou non)

---

## 2. DonnÃ©es utilisÃ©es et aborescence

DonnÃ©es provenant de Kaggle : donnÃ©es synthÃ©tiques sur les ventes par produit, store, date, avec colonnes suivantes :

# Description des colonnes de notre dataframe 


- **Sales Revenue (USD)** : Chiffre d'affaires gÃ©nÃ©rÃ©
- **Units Sold** : QuantitÃ© vendue
- **Discount Percentage** : Pourcentage de remise
- **Marketing Spend (USD)** : DÃ©penses marketing
- **Store ID** : Identifiant du magasin
- **Product Category** : CatÃ©gorie produit
- **Date** : Date de la vente
- **Store Location** : Position gÃ©ographique
- **Day of the Week** : Jour de la semaine
- **Holiday Effect** : Indicateur jours fÃ©riÃ©s

ChargÃ©es dans une table staging_sales sur Fabric Lakehouse.

# Arborescence du projet 


```text

Data-Vault /
â”‚
â”œâ”€â”€ end_to_end_lakehouse/
|   â”œâ”€â”€ README.md
|
|
|   â”œâ”€â”€ Table
â”‚   â”œâ”€â”€ hub_product     /       <- Hub produit avec hash product_id
â”‚   â”œâ”€â”€ hub_store       /       <- Hub store avec hash store_id
â”‚   â””â”€â”€ hub_date        /       <- Hub date avec hash date
â”‚   â”œâ”€â”€ fact_sale_prom  /       <- Table de faits permettant de faire du Power BI
â”‚   â”œâ”€â”€ link_sale       /       <- Link sale avec hash des diffÃ©rents hubs
â”‚   â””â”€â”€ sat_sale        /       <- DonnÃ©es descriptives des transactions
â”‚
|   â”œâ”€â”€ File
â”‚   â”œâ”€â”€ Retail_sales.csv/       <- Fichiers bruts (CSV)
|
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_Bronze.ipynb
â”‚   â”œâ”€â”€ 02_Silver.ipynb
â”‚   â”œâ”€â”€ 03_Gold.ipynb
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ specs_fonctionnelles.md
â”‚   â””â”€â”€ schema_data_vault.drawio
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ tests/
```


---

## 3. ModÃ©lisation Data Vault

Architecture Raw Vault 

Le Data Vault est une architecture moderne dâ€™entrepÃ´t de donnÃ©es (DWH) conÃ§ue pour gÃ©rer des donnÃ©es Ã  grande Ã©chelle, en provenance de sources multiples, tout en assurant :
- lâ€™auditabilitÃ©,

- la traÃ§abilitÃ©,

- lâ€™historisation,

- et la flexibilitÃ©.

"""text

# SchÃ©ma Data Vault â€” ModÃ¨le Retail

Ce modÃ¨le suit la structure Data Vault 2.0 pour une analyse des ventes en lien avec les promotions.  
Il est basÃ© sur 3 entitÃ©s principales : Produit, RÃ©gion, Date.

```text

- Hub : ClÃ© mÃ©tier unique (produit, rÃ©gion, dateâ€¦)
- Link : Fait de liaison entre plusieurs Hubs
- Satellite : Attributs historisÃ©s (mesures, dimensions, contexte)

                  Link_Sale
                  ------------
                  link_sale_hashkey (PK)
                  hub_product_hashkey (FK)
                  hub_region_hashkey  (FK)
                  hub_date_hashkey    (FK)
                  load_dts
                  record_src
                        â–²
                        â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â–²                     â–²                   â–²
      â”‚                     â”‚                   â”‚
      â”‚                     â”‚                   â”‚
      â”‚                     â”‚                   â”‚
      â”‚                     â”‚                   â”‚

Hub_Product           Hub_Region              Hub_Date
--------------     --------------       --------------
hub_product_hashkey(PK)  hub_region_hashkey(PK)  hub_date_hashkey(PK)
product_id              region               date
load_dts                load_dts             load_dts
record_src              record_src           record_src

                            â”‚
                            â”‚
                            â–¼

                          Sat_Sale
                -------------------------------
                link_sale_hashkey (FK)
                units_sold
                sales_revenue
                promo_pct
                marketing_spend
                holiday_flag
                hash_diff
                load_dts
                record_src


```

### 3.1 Hubs

- Hub_Product (clÃ© : product_id)  
- Hub_Store (clÃ© : Store)  
- Hub_Date (clÃ© : date)

### 3.2 Links

- Link_Sale (relation entre produit, store, date)

### 3.3 Satellites

- Sat_Sale (colonnes : units_sold, revenue, promo_pct, marketing_spend, holiday_flag)


> ClÃ©s gÃ©nÃ©rÃ©es par hash MD5, ingestion incrÃ©mentale, respect des bonnes pratiques.

Avantages :
- Historisation native (ex. : changement de prix ou profil client)
- ScalabilitÃ© et adaptation aux changements de structure
- Parfait pour les environnements multi-sources et cloud
  
Limites :
- ComplexitÃ© Ã©levÃ©e
- NÃ©cessite une couche de transformation pour lâ€™analyse BI

---

## 4. Pipelines & Ingestion SQL

Scripts SQL utilisÃ©s pour :

- CrÃ©er les Hubs, Links et Satellites  
- GÃ©nÃ©rer les hashkeys  
- InsÃ©rer les donnÃ©es depuis la staging table

ğŸ“ Voir : ./notebook/silver.ipynb

Vous trouverez le Data lineage dans le schÃ©ma en dessous 

![Mon schÃ©ma](docs/Data_lineage.PNG)
> Des tests de qualitÃ© sont rÃ©alisÃ©s sur la table finale : nulls, ROI nÃ©gatif, doublons.

---

## 5. Business Vault

CrÃ©ation dâ€™une table analytique fact_sales_promo :

- Base : vue PIT_Sale (via jointure Hubs + Link + Sat)  
- Table finale : matÃ©rialisÃ©e en Delta format (CREATE OR REPLACE TABLE ...)  
- Colonnes : product_id, Store, date, promo_flag, revenue, ROI, etc.

Cette table est exposÃ©e Ã  Power BI via Direct Lake ou SQL Endpoint.

ğŸ“ Voir : ./notebook/gold.ipynb

---

## 6. Vue analytique dans Power BI

Visualisations crÃ©Ã©es :

- KPI : CA total, CA promo, ROI moyen  
- Graphique : top 5 produits en promotion (CA)  
- Carte : performances par store  
- Segments interactifs : pÃ©riode, produit, promo ou non

ğŸ“ Fichier Power BI : ./dashboard/report.pbix
---
![Mon schema](docs/PowerBI_retail.PNG)

## 7. Cas dâ€™usage & KPI crÃ©Ã©s

- CA gÃ©nÃ©rÃ© sous promotion  
- ROI promo = revenue / marketing -- Voir le notebook Gold  
- RÃ©partition ventes promo vs non-promo  
- Analyse par Store, par pÃ©riode, par produit

ğŸ“ Les mesures DAX utilisÃ©es sont documentÃ©es dans : ./documents/measures.md

---

## 8. Prochaines Ã©tapes

- Ajouter une granularitÃ© hebdomadaire/mensuelle  
- IntÃ©grer des donnÃ©es de marge (coÃ»ts)  
- Ajouter des prÃ©visions (modÃ¨le ML simple)  
- DÃ©ployer le modÃ¨le avec un pipeline Fabric

---

Â© 2025 â€“ Etienne ZONON | Fullstack Data Bilingue
